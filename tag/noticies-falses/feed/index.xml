<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>Notícies Falses &#8211; La Tecnòloga</title>
	<atom:link href="/tag/noticies-falses/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>La revista tecnològica digital en català</description>
	<lastBuildDate>Fri, 08 May 2020 16:40:52 +0000</lastBuildDate>
	<language>ca</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.8.2</generator>

<image>
	<url>/wp-content/uploads/2019/10/icona_64_fons_trans.png</url>
	<title>Notícies Falses &#8211; La Tecnòloga</title>
	<link>/</link>
	<width>32</width>
	<height>32</height>
</image> 
<site xmlns="com-wordpress:feed-additions:1">177489427</site>	<item>
		<title>Facebook prohibeix els &#8216;deepfakes&#8217;, però permet alguns vídeos manipulats</title>
		<link>/2020/01/08/facebook-prohibeix-els-deepfakes-pero-permet-alguns-videos-manipulats/</link>
					<comments>/2020/01/08/facebook-prohibeix-els-deepfakes-pero-permet-alguns-videos-manipulats/#respond</comments>
		
		<dc:creator><![CDATA[La Tecnòloga]]></dc:creator>
		<pubDate>Wed, 08 Jan 2020 11:38:25 +0000</pubDate>
				<category><![CDATA[Notícies]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Deep Fakes]]></category>
		<category><![CDATA[Facebook]]></category>
		<category><![CDATA[Fake News]]></category>
		<category><![CDATA[Intel·ligència Artificial]]></category>
		<category><![CDATA[Notícies Falses]]></category>
		<guid isPermaLink="false">/?p=633</guid>

					<description><![CDATA[Facebook eliminarà vídeos manipulats amb Intel·ligència Artificial (IA) per distorsionar la realitat, coneguts com a &#8216;deepfakes&#8217;, alguns dels quals són pràcticament indistingibles d&#8217;un vídeo real.&#8230;]]></description>
										<content:encoded><![CDATA[
<p>Facebook eliminarà vídeos manipulats amb Intel·ligència Artificial (IA) per distorsionar la realitat, <a href="/2019/11/13/equipo-e-deepfake-desinformacio/">coneguts com a &#8216;deepfakes&#8217;</a>, alguns dels quals són pràcticament indistingibles d&#8217;un vídeo real. Però&#8230; què són exactament els deepfakes? Tècnicament, són models matemàtics d&#8217;Aprenentatge Automàtic (<em>Machine Learning</em>) que analitzen milers de fotos d&#8217;una persona i aprenen a generar imatges amb la seva cara. Si això es fa per separat amb dues persones i es combinen els dos models, s&#8217;aconsegueix crear un model nou -o sigui, una imatge nova-, que és el deepfake.</p>



<p>D&#8217;aquesta manera, <a rel="noreferrer noopener" aria-label="En un comunicat, la companyia explica que suprimirà o etiquetarà els vídeos fets amb la intenció d'enganyar (opens in a new tab)" href="https://about.fb.com/news/2020/01/enforcing-against-manipulated-media/" target="_blank">Facebook explica en un comunicat que suprimirà o etiquetarà els vídeos fets amb la intenció d&#8217;enganyar</a>; o sigui, aquelles falsificacions que als ulls de qualsevol persona podrien semblar reals, només si han estat fets amb Intel·ligència Artificial. </p>



<p>La companyia, això sí, explica que no implementarà una política de retirada general dels vídeos manipulats marcats com a falsos pels verificadors (<em>fact-checkers</em>). D&#8217;aquesta manera, la nova directriu no s&#8217;aplicarà als vídeos satírics, ni als que només s&#8217;ha canviat l&#8217;ordre dels mots o s&#8217;han suprimit. Així, l&#8217;empresa de Mark Zuckerberg no prohibirà un vídeo com el de la presidenta de la Cambra de Representants dels Estats Units, Nancy Pelosi, que va editar-se (sense fer servir IA) per fer com si anés borratxa, i que va fer-se viral l&#8217;estiu passat.</p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"><iframe loading="lazy" title="Doctored Pelosi video highlights the threat of deepfake tech" width="1160" height="653" src="https://www.youtube.com/embed/EfREntgxmDs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div></figure>



<p>Les reaccions al vídeo de Pelosi van ser diverses. Uns, condemnaren Facebook per no prohibir el vídeo. Altres, argumentaren que eliminar-lo assentava un perillós precedent per censurar paròdies polítiques o dissidents.</p>



<p>Sens dubte, els vídeos falsos, editats amb eines a l&#8217;abast de la majoria, constitueixen una amenaça important per als polítics immersos en una cursa electoral, per exemple. Però la pregunta, aleshores, és: qui decideix què és satíric i què és enganyós? On és la línia que separa aquestes dues categories? Cal prohibir els vídeos falsos que mostren, posem per cas, una falsa declaració racista? I això, Facebook, no ho aclareix.</p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"><iframe loading="lazy" title="¿Cómo se hace un &#039;deepfake&#039;?" width="1160" height="653" src="https://www.youtube.com/embed/nU0r-5vJUH0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div><figcaption>En aquest vídeo, on es mostra com es fa un Deepfake, el secretari general de Vox, Javier Ortega Smith, es transforma en l&#8217;expresident de Ciutadans, Albert Rivera.</figcaption></figure>



<p></p>
]]></content:encoded>
					
					<wfw:commentRss>/2020/01/08/facebook-prohibeix-els-deepfakes-pero-permet-alguns-videos-manipulats/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">633</post-id>	</item>
		<item>
		<title>Deepfakes per riure&#8230; i enganyar</title>
		<link>/2019/11/13/equipo-e-deepfake-desinformacio/</link>
					<comments>/2019/11/13/equipo-e-deepfake-desinformacio/#respond</comments>
		
		<dc:creator><![CDATA[Anna Schnabel]]></dc:creator>
		<pubDate>Tue, 12 Nov 2019 23:21:28 +0000</pubDate>
				<category><![CDATA[Notícies]]></category>
		<category><![CDATA[Aprenentatge Automàtic]]></category>
		<category><![CDATA[Deep Fakes]]></category>
		<category><![CDATA[Fake News]]></category>
		<category><![CDATA[Falsificacions]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Notícies Falses]]></category>
		<guid isPermaLink="false">/?p=349</guid>

					<description><![CDATA[L&#8217;Equip E, amb E d&#8217;Espanya és el títol d&#8217;un vídeo publicat fa 24 hores, però que ja acumula 700.000 reproduccions. Es tracta d&#8217;una paròdia de&#8230;]]></description>
										<content:encoded><![CDATA[
<p><em>L&#8217;Equip E, amb E d&#8217;Espanya</em> és el títol d&#8217;un vídeo publicat fa 24 hores, però que ja acumula 700.000 reproduccions. Es tracta d&#8217;una paròdia de 2:35 minuts que, amb la tecnologia dels Deepfakes, connecta la mítica sèrie &#8216;Equipo A&#8217; amb les principals cares de la política espanyola que han competit a les eleccions de l&#8217;11 de novembre. </p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"><iframe loading="lazy" title="EL EQUIPO E, con E de España [DeepFake]" width="1160" height="653" src="https://www.youtube.com/embed/dj5M4s-cdAw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div><figcaption>[DeepFake] El Equipo E, con E de España.</figcaption></figure>



<p>Pablo Casado dona vida al coronel Hannibal, Pablo Iglesias es transforma -amb melena inclosa- en Amanda Allen, Pedro Sánchez encarna el tinent Templeton Peck, Santiago Abascal interpreta a B.A. Baracus, mentre Albert Rivera hi apareix com el capità Murdock.  </p>



<p>És el dotzè vídeo del <a rel="noreferrer noopener" aria-label="canal de Youtube 'Face to Fake' (s'obre en una nova pestanya)" href="https://www.youtube.com/channel/UCg9FVKnbCqfX-OuIFVgEZgw/videos" target="_blank">canal de Youtube &#8216;Face to Fake&#8217;</a>, que utilitza la Intel·ligència Artificial per a divertir els espectadors, tal com s&#8217;autodefineixen els creadors. I sembla que ho han aconseguit. Els vídeos manipulats no són novetat, tot i que sorprèn el realisme que guanyen dia a dia. Altres canals de Youtube, com <a rel="noreferrer noopener" aria-label="Ctrl Shift Face (s'obre en una nova pestanya)" href="https://www.youtube.com/channel/UCKpH0CKltc73e4wh0_pgL3g" target="_blank">Ctrl Shift Face</a>, van més enllà i modifiquen la cara dels actors a temps real.</p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"><iframe loading="lazy" title="Bill Hader channels Tom Cruise [DeepFake]" width="1160" height="653" src="https://www.youtube.com/embed/VWrhRBb-1Ig?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div><figcaption>[DeepFake] L&#8217;actor Bill Hader es transforma en Tom Cruise.</figcaption></figure>



<h5>No tot són rialles</h5>



<p>Més enllà de les sàtires, la mateixa tecnologia dels Deepfakes té usos professionals i Hollywood ja <a rel="noreferrer noopener" aria-label="utilitza la mateixa tecnologia per rejovenir actors (s'obre en una nova pestanya)" href="https://www.lavanguardia.com/tecnologia/20191106/471413348212/cine-series-hbo-inteligencia-artificial-productoras.html" target="_blank">les fa servir per rejovenir actors</a>. Allò cert, però, és que els Deepfakes han emergit de la cara més fosca d&#8217;Internet, on <a rel="noreferrer noopener" aria-label="s'utilitzaven sobretot per a falsificacions pornogràfiques (s'obre en una nova pestanya)" href="https://www.technologyreview.com/f/614485/deepfake-porn-deeptrace-legislation-california-election-disinformation/" target="_blank">s&#8217;utilitzen sobretot per a falsificacions pornogràfiques</a>. A més a més, es fan servir per a falsejar notícies. De fet, els vídeos manipulats han aterrat a les xarxes socials amb més pena que glòria. Facebook i Twitter van bullir quan Obama pronuncià una frase capaç de generar rius de polèmica: &#8220;El president Trump és totalment idiota&#8221;. En realitat, mai ho va dir. </p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"><iframe loading="lazy" title="You Won’t Believe What Obama Says In This Video! &#x1f609;" width="1160" height="653" src="https://www.youtube.com/embed/cQ54GDm1eL0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div></figure>



<p>Altres personatges famosos com <a rel="noreferrer noopener" aria-label="Donald Trump (s'obre en una nova pestanya)" href="https://www.youtube.com/watch?v=dJJLqXVpVGY" target="_blank">Donald Trump</a> o <a rel="noreferrer noopener" aria-label="l'actriu Scarlett Johansson (s'obre en una nova pestanya)" href="https://www.youtube.com/watch?v=Eongq2cl8-I" target="_blank">l&#8217;actriu Scarlett Johansson</a> han estat blancs d&#8217;imatges manipulades que han aixecat polseguera i s&#8217;han viralitzat tan bon punt s&#8217;han fet públics. Parlem de vídeos que mostren gestos, expressions i moviments que, pel seu realisme, arriben a confondre l&#8217;espectador. I tenen tots els números de convertir-se en la nova gran font de desinformació.</p>



<h5>Com es genera un Deepfake?</h5>



<p>Els Deepfakes es creen mitjançant algoritmes d&#8217;Intel·ligència Artificial que observen i registren patrons de moviment del rostre d&#8217;algú a partir d&#8217;un vídeo real i, després, els recreen utilitzant les faccions de la cara d&#8217;algú altre que han memoritzat prèviament. El terme &#8220;Deep&#8221; fa referència al concepte del &#8220;Deep Learning&#8221; (Aprenentatge Profund, en català), una família d&#8217;algoritmes d&#8217;Aprenentatge Automàtic (Machine Learning, en anglès) que s’inspira en el funcionament del cervell humà. </p>



<p>Existeix algun sistema fiable per detectar els vídeos manipulats? La investigació es desplega a les millors universitats i als departaments de recerca de les grans empreses tecnològiques però, de moment, els algoritmes de detecció no estan tant perfeccionats com per enxampar-los tot d&#8217;una. De fet, Facebook va admetre el setembre que <a rel="noreferrer noopener" aria-label="no era capaç d'identificar els Deep Fake més sofisticats (s'obre en una nova pestanya)" href="https://ai.facebook.com/blog/deepfake-detection-challenge" target="_blank">no era capaç d&#8217;identificar els Deepfakes més sofisticats</a> perquè la tecnologia que els produeix millora massa ràpid. </p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"><iframe loading="lazy" title="Taxi Driver starring Al Pacino [DeepFake]" width="1160" height="653" src="https://www.youtube.com/embed/9NkKj0aNB0s?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div><figcaption>[DeepFake] Al Pacino substitueix Robert De Niro a Taxi Driver.</figcaption></figure>



<p>En aquest context, hi ha més preguntes que respostes: Com frenar les mentides, si corren més que les veritats? Fins a quin punt poden arribar a perfeccionar-se els Deepfakes? Què passarà quan es viralitzi la imatge d&#8217;un líder polític cometent un acte deshonest, però no es pugui saber si és real o fals? Quins continguts es viralitzaran quan tothom pugui fer vídeos falsos des de qualsevol ordinador? </p>



<p>De moment, la legislació no és clara sobre els Deepfakes, tot i que un grapat d&#8217;Estats ja han pres partit. Fa menys d&#8217;una setmana que <a rel="noreferrer noopener" aria-label="Califòrnia ha il·legalitzat la creació i la distribució (s'obre en una nova pestanya)" href="https://www.theguardian.com/us-news/2019/oct/07/california-makes-deepfake-videos-illegal-but-law-may-be-hard-to-enforce" target="_blank">Califòrnia ha il·legalitzat la creació i la distribució</a> de Deepfakes amb l&#8217;argument de protegir els votants de la desinformació.</p>
]]></content:encoded>
					
					<wfw:commentRss>/2019/11/13/equipo-e-deepfake-desinformacio/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">349</post-id>	</item>
		<item>
		<title>OpenAI allibera un generador intel·ligent de text que considerava massa perillós</title>
		<link>/2019/11/12/openai-allibera-un-generador-intelligent-de-text-que-considerava-massa-perillos/</link>
					<comments>/2019/11/12/openai-allibera-un-generador-intelligent-de-text-que-considerava-massa-perillos/#comments</comments>
		
		<dc:creator><![CDATA[Carles Sala]]></dc:creator>
		<pubDate>Tue, 12 Nov 2019 12:13:44 +0000</pubDate>
				<category><![CDATA[Notícies]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Fake News]]></category>
		<category><![CDATA[Generador de text]]></category>
		<category><![CDATA[Intel·ligència Artificial]]></category>
		<category><![CDATA[Notícies Falses]]></category>
		<category><![CDATA[OpenAI]]></category>
		<category><![CDATA[Text generator]]></category>
		<guid isPermaLink="false">/?p=312</guid>

					<description><![CDATA[L’empresa de recerca OpenAI ha fet pública aquesta setmana la versió més completa del seu sistema GPT-2 de generació de text sintètic, que inicialment havia&#8230;]]></description>
										<content:encoded><![CDATA[
<p>L’empresa de recerca OpenAI ha fet pública aquesta setmana <a href="https://openai.com/blog/gpt-2-1-5b-release/" target="_blank" rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)">la versió més completa del seu sistema GPT-2</a> de generació de text sintètic, que inicialment havia decidit no fer accessible al públic per evitar usos malintencionats.&nbsp;</p>



<p>Al febrer, la companyia del propietari de Tesla, Elon Musk, <a rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)" href="https://openai.com/blog/better-language-models/" target="_blank">anunciava la creació d’un generador intel·ligent de text model d’Intel·ligència Artificial</a> capaç de generar escrits artificials pràcticament indistingibles dels que hauria escrit un humà. Al mateix article, però, els autors mostraven preocupació per la possibilitat que l&#8217;eina es fes servir de manera maliciosa, i van considerar que era “massa perillosa” per alliberar-la. Segons els investigadors, podia utilitzar-se per generar ràpidament grans quantitats de text de tota mena com notícies falses o propaganda ideològica, i fins i tot imitar un estil de redacció i fer-se passar per algú en concret. </p>



<p>Per aquest motiu, durant els últims mesos, OpenAI ha seguit una estratègia de publicació per etapes, alliberant-ne versions reduïdes cada pocs mesos, i observant l’ús que se’n feia. La publicació d&#8217;aquesta setmana ha estat la darrera d’aquest procés i, en un comunicat, els autors deixen clar que no han observat “cap mal ús evident” dels models alliberats fins ara.</p>



<h2>Una nova generació de models</h2>



<p>El polèmic model forma part d’una nova família de generadors de text basats en un concepte anomenat ‘Transformer’, que han demostrat ser altament efectius a l’hora de produir textos coherents rebent ben poca informació d’entrada. Per entrenar-lo, s’ha fet servir el text de més de 8 milions de pàgines web (és a dir, 40GB de text), optimitzant internament més de 1.500 milions de paràmetres en un procés d’Aprenentatge Profund (Deep Learning, en anglès). El model sencer ha estat publicat <a href="https://github.com/openai/gpt-2" target="_blank" rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)">al repositori de GitHub d’OpenAI</a>, on hi trobem tant el codi font com els seus paràmetres optimitzats, amb instruccions detallades de com fer-lo servir.</p>



<p>Però no només els programadors poden utilitzar-lo. Existeix, a més, una versió de prova en línia al web <a rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)" href="https://talktotransformer.com/" target="_blank">TalkToTransformer.com</a>. En aquesta versió, l’usuari introdueix les primeres paraules o frases d’un text i el sistema és capaç de completar-lo de manera creativa, tot mantenint la coherència i l’estil de redacció. D’aquesta manera, si proporcionem les primeres paraules d’una història, el model n’escriu la continuació. Si escrivim un fragment de diàleg, l&#8217;eina estén la conversa i fins i tot hi introdueix nous personatges.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img loading="lazy" src="/wp-content/uploads/2019/11/image-2.png" alt="" class="wp-image-318" width="557" height="497" srcset="/wp-content/uploads/2019/11/image-2.png 626w, /wp-content/uploads/2019/11/image-2-300x268.png 300w" sizes="(max-width: 557px) 100vw, 557px" /><figcaption>GPT-2 és capaç d&#8217;entendre el context i fins i tot continuar el diàleg entre personatges</figcaption></figure></div>



<p>Però, tot i que el model pot produir grans resultats, no és perfecte. Després de diverses proves, alguns errors afloren i s’acaben produïnt diàlegs incoherents o sobtats canvis de tema.<br></p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img loading="lazy" src="/wp-content/uploads/2019/11/image-1.png" alt="" class="wp-image-315" width="496" height="383" srcset="/wp-content/uploads/2019/11/image-1.png 605w, /wp-content/uploads/2019/11/image-1-300x232.png 300w, /wp-content/uploads/2019/11/image-1-520x400.png 520w" sizes="(max-width: 496px) 100vw, 496px" /><figcaption>GPT-2 és fins i tot capaç de generar codi font amb una certa coherència, però sense cap utilitat real.</figcaption></figure></div>



<h2>Una publicació polèmica</h2>



<p>Quan OpenAI va anunciar fa nou mesos que no faria públic el model definitiu, es <a rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)" href="https://anima-ai.org/2019/02/18/an-open-and-shut-case-on-openai/" target="_blank">va aixecar una gran polèmica, sobretot a les xarxes socials</a>. Per un cantó, alguns argumentaven que no tenia sentit mantenir-lo en privat després de publicar els resultats, ja que qualsevol altre equip podia reproduir-los. Per contra, <a rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)" href="https://thegradient.pub/openai-please-open-source-your-language-model/" target="_blank">mantenir-lo en secret impedia que altres investigadors aprofundissin en la manera de pal·liar el possible mal ús de l’eina</a>. Mentrestant, altres defensaven que en realitat s’havia presentat com a perillosa per seguir una estratègia de <a rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)" href="http://deliprao.com/archives/314" target="_blank">publicitat mitjançant la polèmica</a>.</p>



<p>També n’hi havia que consideraven justificats <a href="https://thegradient.pub/openai-shouldnt-release-their-full-language-model/" target="_blank" rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)">els motius d’OpenAI per limitar la publicació</a>, sobretot per la recent proliferació de <a rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)" href="https://ca.wikipedia.org/wiki/Deepfake" target="_blank">DeepFakes</a> i el seu ús maliciós. Al cap i a la fi, <a rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)" href="https://www.youtube.com/watch?v=4GdWD0yxvqw" target="_blank">les disculpes de Jon Snow per la sisena temporada de Joc de Trons</a> semblaven inofensives, però el vídeo d’<a rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)" href="https://www.youtube.com/watch?v=cQ54GDm1eL0" target="_blank">Obama insultant Trump</a> o el <a rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)" href="https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-deepfake-was-used-to-scam-a-ceo-out-of-243000/#78eb3c2f2241" target="_blank">robatori de 220.000€ a una empresa alemanya per mitjà de la falsificació de la veu del director executiu</a> resultaven més preocupants.</p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"><iframe loading="lazy" title="You Won’t Believe What Obama Says In This Video! &#x1f609;" width="1160" height="653" src="https://www.youtube.com/embed/cQ54GDm1eL0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div><figcaption>Demostració del possible mal ús dels DeepFakes</figcaption></figure>



<h2>Conclusions d’OpenAI</h2>



<p>Juntament amb el model GPT-2, OpenAI ha fet públiques les conclusions de l’estudi que ha desenvolupat durant els últims mesos, a mesura que anava publicant les diferents versions:</p>



<ol><li><strong>GPT-2 produeix resultats convincents.</strong> A mida que s’alliberaven les versions, la Universitat de Cornell feia enquestes per comprovar si els textos generats resultaven prou convincents. El model complet té una credibilitat de 6.91 sobre 10, mentre que les versions anteriors, més reduïdes, van tenir puntuacions d’entre 6.07 i 6.72.</li><li><strong>GPT-2 pot ser modificat per fer-ne un mal ús.</strong> Investigadors del Middlebury Institute of International Studies’ Center on Terrorism, Extremism, and Counterterrorism (CTEC) han demostrat que el sistema es pot modificar per tal de generar massivament propaganda sintètica convincent enfocada a ideologies extremistes, com la supremacia blanca o el gihadisme islàmic. Tot i així, malgrat que els sistemes de detecció automàtics encara no són prou robustos, asseguren que el desenvolupament d&#8217;eines que assisteixin als humans a detectar textos sintètics és viable.</li><li><strong>És difícil de detectar.</strong> OpenAI ha desenvolupat un altre model especialitzat en la detecció de textos generats per GPT-2. Aquest assoleix un encert del 95%, però els investigadors consideren que aquest resultat encara no és prou bo com per considerar que té prous garanties. Per aquest motiu, també <a rel="noreferrer noopener" aria-label=" (s'obre en una nova pestanya)" href="https://github.com/openai/gpt-2-output-dataset/tree/master/detector" target="_blank">han publicat el model de detecció</a>, així com les dades que han fet servir per al seu desenvolupament, amb l’esperança que altres investigadors puguin millorar els seus resultats.</li><li><strong>Encara no han detectat cap mal ús.</strong> Tot i que consideren que el sistema podria utilitzar-se amb males intencions, encara no han observat cap cas. A més, admeten que, tot i no fer públic el model complet, qualsevol interessat en fer-ne un mal ús podria reproduir els seus passos i generar la seva pròpia versió.</li><li><strong>Cal estandaritzar l’estudi del biaix.</strong> OpenAI ha estat intentant avaluar si el model està esbiaixat i, per tant, pot generar <a href="/2019/11/11/algoritmes-esbiaixats-maquines-que-no-fan-justicia/">textos amb desviacions ètiques per raons de gènere, raça o religió</a>. Els investigadors han fet públics els resultats d’aquesta anàlisi, però també subratllen que és insuficient degut a l’absència de metodologies i marcs de treball estandarditzats per fer aquest tipus d’avaluacions.</li></ol>



<p>Finalment, OpenAI remarca la necessitat de seguir treballant amb la comunitat científica per garantir una publicació responsable dels resultats, de cara a no facilitar el mal ús de models d’Intel·ligència Artificial com GPT-2.</p>



<h5>Referències: </h5>



<ul><li><a href="https://www.theverge.com/2019/11/7/20953040/openai-text-generation-ai-gpt-2-full-model-release-1-5b-parameters">https://www.theverge.com/2019/11/7/20953040/openai-text-generation-ai-gpt-2-full-model-release-1-5b-parameters</a></li><li><a href="https://openai.com/blog/gpt-2-1-5b-release/">https://openai.com/blog/gpt-2-1-5b-release/</a></li><li><a href="https://openai.com/blog/better-language-models/">https://openai.com/blog/better-language-models/</a></li><li><a href="https://talktotransformer.com/">https://talktotransformer.com/</a></li><li><a href="https://github.com/openai/gpt-2">https://github.com/openai/gpt-2</a></li><li><a href="https://github.com/openai/gpt-2-output-dataset">https://github.com/openai/gpt-2-output-dataset</a></li></ul>



<p><br></p>
]]></content:encoded>
					
					<wfw:commentRss>/2019/11/12/openai-allibera-un-generador-intelligent-de-text-que-considerava-massa-perillos/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">312</post-id>	</item>
		<item>
		<title>Decàleg per enxampar notícies falses</title>
		<link>/2019/10/14/enxampa-les-fake-news/</link>
					<comments>/2019/10/14/enxampa-les-fake-news/#respond</comments>
		
		<dc:creator><![CDATA[Anna Schnabel]]></dc:creator>
		<pubDate>Mon, 14 Oct 2019 15:02:57 +0000</pubDate>
				<category><![CDATA[Notícies]]></category>
		<category><![CDATA[Critical Thinking]]></category>
		<category><![CDATA[Fake News]]></category>
		<category><![CDATA[Notícies Falses]]></category>
		<category><![CDATA[Pensament Crític]]></category>
		<guid isPermaLink="false">/?p=48</guid>

					<description><![CDATA[Espanya té el dubtós honor de ser el país d&#8217;Europa on més proliferen les &#8216;fake news&#8217; Si no fos per les notícies falses, potser Donald&#8230;]]></description>
										<content:encoded><![CDATA[
<h3>Espanya té el dubtós honor de ser el país d&#8217;Europa on més proliferen les &#8216;fake news&#8217;</h3>



<p>Si no fos per les notícies falses, potser Donald Trump no seria president i Gran Bretanya no estaria a un pas d&#8217;abandonar la Unió Europea. Així ho mostra <a rel="noreferrer noopener" aria-label="el documental The Great Hack (s'obre en una nova pestanya)" href="https://www.netflix.com/title/80117542" target="_blank">el documental The Great Hack</a>, que explica el paper de l&#8217;empresa Cambridge Analytica al tauler polític: va accedir a les dades de milions d&#8217;usuaris de Facebook i les va utilitzar per crear continguts a mida, sovint falsos, amb la intenció de manipular els votants. D&#8217;aquesta manera, primer predien la personalitat i les inclinacions dels usuaris i, després, els enviaven missatges personalitzats, per exemple qualificant a Hillary Clinton de corrupta, encara que no fos veritat.</p>



<p>I tot això passa a l&#8217;alba de la redacció intel·ligent, quan es multipliquen els sistemes de generació automàtica de text capaços de crear continguts d&#8217;allò més plausibles. <a rel="noreferrer noopener" href="https://www.dail.es/leorobot/" target="_blank">LeoRobot</a> és el primer del mercat que processa el llenguatge natural en castellà. I n&#8217;hi ha molts més, com <a rel="noreferrer noopener" href="https://textio.com/" target="_blank">Textio</a>, <a rel="noreferrer noopener" href="https://www4.ax-semantics.com/" target="_blank">AX Semantics</a> o<a rel="noreferrer noopener" href="https://talktotransformer.com/" target="_blank"> Talktotransformer, d&#8217;OpenAI</a>, que originalment va ser considerat massa perillós per ser utilitzat pel públic.  </p>



<p>De moment, Espanya té el dubtós honor de ser el país d&#8217;Europa on més proliferen les notícies falses. <a rel="noreferrer noopener" aria-label="Segons un estudi (s'obre en una nova pestanya)" href="https://www.ipsos.com/sites/default/files/ct/news/documents/2018-09/fake-news-filter-bubbles-post-truth-and-trust.pdf" target="_blank">Segons un estudi</a>, el 57% d&#8217;espanyols admet haver-se cregut alguna vegada una informació falsa, cosa que ens situa a la cinquena posició del marcador mundial. <a rel="noreferrer noopener" aria-label="Un altre estudi  (s'obre en una nova pestanya)" href="https://www.gartner.com/en/newsroom/press-releases/2017-10-03-gartner-reveals-top-predictions-for-it-organizations-and-users-in-2018-and-beyond" target="_blank">Un altre estudi </a>sosté que, al 2022, la majoria consumirem més &#8216;fake news&#8217; que informacions certes. </p>



<h5>El decàleg</h5>



<p>No ens ho posen fàcil ni l&#8217;allau d&#8217;informació diària que ens arriba, ni la impossibilitat de contrastar totes les dades, ni la rapidesa amb què tot circula a les xarxes socials, per on discorre un flux continu de notícies d&#8217;aparença neutre. En un escenari que és brou de cultiu per a les decisions basades en mentides, mantenir una mirada crítica és essencial:</p>



<ol><li>Llegeix la notícia sencera i no et quedis amb el titular. No et creguis cap notícia immediatament, ni la comparteixis tan bon punt l&#8217;has rebuda.</li><li>Dubta de les notícies amb faltes d&#8217;ortografia o que no estan firmades.</li><li>Desconfia de les notícies que et produeixin un gran impacte emocional.</li><li>Descobreix la font. Desenvolupa l&#8217;hàbit d&#8217;investigar d&#8217;on ha sortit la informació. Quan una notícia és certa, és més probable que citi fonts, inclogui enllaços i citi documents oficials.</li><li> Esbrina l&#8217;autoria. Busca el nom del mitjà o l&#8217;autor a Google per veure què més ha escrit aquesta persona. </li><li>Busca el titular a Google. Si la notícia és certa, és probable que altres mitjans també l&#8217;hagin publicat. Si no hi és, cerca les paraules clau o el titular seguit de la paraula &#8220;bola&#8221;, &#8220;bulo&#8221; o &#8220;hoax&#8221; (en català, en castellà o en anglès, respectivament). Si es tracta d&#8217;una bola coneguda, apareixerà immediatament a la cerca. </li><li>Busca les dades que cita.</li><li>Verifica el context, com la data de publicació. Descontextualitzar una informació també és desinformar. </li><li>Si t&#8217;han enviat la notícia, pregunta a qui te l&#8217;ha fet arribar d&#8217;on l&#8217;ha tret. </li><li>Has rebut una imatge que conta una història? <a rel="noreferrer noopener" aria-label="Fes una cerca inversa d'imatges (s'obre en una nova pestanya)" href="https://images.google.com/" target="_blank">Fes una cerca inversa d&#8217;imatges</a> i comprova si altres llocs l&#8217;han publicat. </li></ol>



<p>Per altra banda, existeixen algunes eines per detectar continguts falsos. Per exemple, podem utilitzar <a rel="noreferrer noopener" aria-label="'Fets o Fakes', el propi servei de 'fact-checking' (verificació de fets) de Catalunya Ràdio (s'obre en una nova pestanya)" href="https://www.ccma.cat/catradio/catalunya-migdia/fetsofakes/fitxa/6393/" target="_blank">&#8216;Fets o Fakes&#8217;, que és el propi servei de &#8216;fact-checking&#8217; (verificació de fets) de Catalunya Ràdio</a>. Està format per un equip transversal de periodistes del departament d&#8217;Informatius que cerquen les fonts primàries de la informació i qualsevol hi pot contactar enviant una petició al correu fetsofakes@ccma.cat o escrivint un WhatsApp al 677070940. Per altra banda, també <a rel="noreferrer noopener" href="http://www.verificat.cat/" target="_blank">trobem Verificat, una altra plataforma de &#8216;fact-checkin&#8217;</a> catalana. Davant d&#8217;una sospita, els ciutadans poden contactar-hi per Whatsapp al 666382694. Aleshores, Verificat busca l&#8217;origen de la notícia, pregunta a la font original, consulta fonts alternatives i contextualitza la informació. I encara tenim més eines al nostre abast. També ens pot ser útil la pàgina de denúncia de notícies falses <a rel="noreferrer noopener" href="https://maldita.es/" target="_blank">Maldita.es</a>, que fins i tot ofereix una <a rel="noreferrer noopener" href="https://maldita.es/descarga-la-extension-de-maldito-bulo-para-google-chrome-y-firefox/" target="_blank">extensió per alertar l&#8217;usuari</a> quan navega per una pàgina poc fiable. A més, <a rel="noreferrer noopener" href="https://es.wikitribune.com/" target="_blank">Wikitribune</a> és un web de notícies on periodistes investiguen i informen sobre notícies i els voluntaris supervisen els articles. El <a rel="noreferrer noopener" aria-label="Fake News Detector (s'obre en una nova pestanya)" href="https://fakenewsdetector.org/en" target="_blank">Fake News Detector</a> permet detectar i marcar les notícies falses, els <a rel="noreferrer noopener" aria-label="pescaclics (s'obre en una nova pestanya)" href="https://ca.wikipedia.org/wiki/Pescaclics" target="_blank">pescaclics</a> o les notícies extremadament esbiaixades. I el <a rel="noreferrer noopener" aria-label="News Cracker (s'obre en una nova pestanya)" href="https://chrome.google.com/webstore/detail/newscracker/lmpfanpnpoaegbafkodbifallmfcncpb" target="_blank">News Cracker</a> és una tecnologia d&#8217;<a rel="noreferrer noopener" aria-label="aprenentatge automàtic (s'obre en una nova pestanya)" href="https://ca.wikipedia.org/wiki/Aprenentatge_autom%C3%A0tic" target="_blank">aprenentatge automàtic</a> que permet puntuar de l&#8217;1 al 10 la qualitat de qualsevol article, contingut o publicació, en funció de la seva veracitat. </p>



<h5>Pensament crític</h5>



<p>Un pensament crític també és clau per evitar que ens colin mentides o mitges veritats. Però, per començar, hem de tenir en compte els nostres propis biaixos, i hem d&#8217;entendre que podem estar equivocats. Per això, és recomanable consultar diferents mitjans i no sempre el mateix, per evitar estancar-nos en una bombolla informativa. Al cap i a la fi, el nostre punt de vista pot no ser objectiu, ni l&#8217;únic vàlid al món. També pot ser útil valorar si una notícia afavoreix determinats interessos, i si l&#8217;argumentació està ben fonamentada o, al contrari, si amaga algun <a rel="noreferrer noopener" aria-label="tipus de fal·làcia (s'obre en una nova pestanya)" href="http://www.xtec.cat/~lvallmaj/preso/fal-logi.htm" target="_blank">tipus de fal·làcia</a>. </p>



<p>Som, en teoria, més lliures que mai i, no obstant, en la pràctica, som blancs d&#8217;intents constants de manipulació. &#8220;L’element principal del control social és l’estratègia de distracció, desviar l’atenció del públic de les qüestions determinants per les elits polítiques i econòmiques&#8221;, sostenia <a rel="noreferrer noopener" aria-label="Noam Chomsky, que sintetitzà en un decàleg (s'obre en una nova pestanya)" href="https://noam-chomsky.tumblr.com/post/13867896307/noam-chomsky-10-strategies-of-manipulation-by" target="_blank">Noam Chomsky, que sintetitzà en un decàleg</a> les estratègies més comunes dels mitjans de comunicació per modelar l&#8217;opinió pública. Velles tècniques que, malgrat tot, continuen fent-nos caure en l&#8217;engany.</p>
]]></content:encoded>
					
					<wfw:commentRss>/2019/10/14/enxampa-les-fake-news/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">48</post-id>	</item>
	</channel>
</rss>
